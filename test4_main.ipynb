{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-16T17:19:53.923982Z",
     "iopub.status.busy": "2025-09-16T17:19:53.923730Z",
     "iopub.status.idle": "2025-09-16T17:20:00.809766Z",
     "shell.execute_reply": "2025-09-16T17:20:00.809205Z",
     "shell.execute_reply.started": "2025-09-16T17:19:53.923962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T17:20:00.811413Z",
     "iopub.status.busy": "2025-09-16T17:20:00.810957Z",
     "iopub.status.idle": "2025-09-16T17:20:00.866733Z",
     "shell.execute_reply": "2025-09-16T17:20:00.865995Z",
     "shell.execute_reply.started": "2025-09-16T17:20:00.811386Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 1. Configuration ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CHECKPOINT_PATH = \"/kaggle/input/new-trained-models/trained_model/best_model.pth.tar\"\n",
    "VIDEO_PATH = \"/kaggle/input/test-clips/clip4.mp4\"\n",
    "OUTPUT_VIDEO_PATH = \"/kaggle/working/final-output-clip4.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T17:20:00.867822Z",
     "iopub.status.busy": "2025-09-16T17:20:00.867518Z",
     "iopub.status.idle": "2025-09-16T17:20:00.882409Z",
     "shell.execute_reply": "2025-09-16T17:20:00.881746Z",
     "shell.execute_reply.started": "2025-09-16T17:20:00.867794Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IMG_HEIGHT, IMG_WIDTH = 288, 512\n",
    "THRESHOLD = 0.4\n",
    "USE_IMAGENET_NORM = True\n",
    "SMOOTHING_WINDOW = 7 # Frames to average over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T17:20:00.884395Z",
     "iopub.status.busy": "2025-09-16T17:20:00.884121Z",
     "iopub.status.idle": "2025-09-16T17:20:00.897516Z",
     "shell.execute_reply": "2025-09-16T17:20:00.896906Z",
     "shell.execute_reply.started": "2025-09-16T17:20:00.884378Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===================== U-Net Model =====================\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
    "        super(UNet, self).__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Downsampling\n",
    "        for feature in features:\n",
    "            self.downs.append(self.double_conv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.double_conv(features[-1], features[-1] * 2)\n",
    "\n",
    "        # Upsampling\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2))\n",
    "            self.ups.append(self.double_conv(feature * 2, feature))\n",
    "\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx // 2]\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = F.interpolate(x, size=skip_connection.shape[2:])\n",
    "            x = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx + 1](x)\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def double_conv(in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T17:20:00.898373Z",
     "iopub.status.busy": "2025-09-16T17:20:00.898197Z",
     "iopub.status.idle": "2025-09-16T17:20:00.915117Z",
     "shell.execute_reply": "2025-09-16T17:20:00.914518Z",
     "shell.execute_reply.started": "2025-09-16T17:20:00.898359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- MODIFIED: Lane History now stores and averages polynomial fits ---\n",
    "class LaneHistory:\n",
    "    def __init__(self, queue_size=SMOOTHING_WINDOW, tolerance=10):\n",
    "        self.left_fit = deque(maxlen=queue_size)\n",
    "        self.right_fit = deque(maxlen=queue_size)\n",
    "        self.missed_frames = 0\n",
    "        self.tolerance = tolerance\n",
    "\n",
    "    def add_fit(self, left_fit, right_fit):\n",
    "        if left_fit is None or right_fit is None:\n",
    "            self.missed_frames += 1\n",
    "        else:\n",
    "            self.left_fit.append(left_fit)\n",
    "            self.right_fit.append(right_fit)\n",
    "            self.missed_frames = 0\n",
    "\n",
    "    def get_average_fit(self):\n",
    "        if self.missed_frames > self.tolerance:\n",
    "            self.left_fit.clear()\n",
    "            self.right_fit.clear()\n",
    "            return None, None\n",
    "        if not self.left_fit or not self.right_fit:\n",
    "            return None, None\n",
    "            \n",
    "        avg_left_fit = np.mean(self.left_fit, axis=0)\n",
    "        avg_right_fit = np.mean(self.right_fit, axis=0)\n",
    "        return avg_left_fit, avg_right_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T17:20:00.915845Z",
     "iopub.status.busy": "2025-09-16T17:20:00.915688Z",
     "iopub.status.idle": "2025-09-16T17:20:03.900023Z",
     "shell.execute_reply": "2025-09-16T17:20:03.899425Z",
     "shell.execute_reply.started": "2025-09-16T17:20:00.915831Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading trained model...\n",
      "✅ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Load Model (same as your version) ---\n",
    "print(\"=> Loading trained model...\")\n",
    "model = UNet(in_channels=3, out_channels=1).to(DEVICE)\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model.eval()\n",
    "print(\"✅ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T17:20:03.900981Z",
     "iopub.status.busy": "2025-09-16T17:20:03.900766Z",
     "iopub.status.idle": "2025-09-16T17:20:03.905267Z",
     "shell.execute_reply": "2025-09-16T17:20:03.904672Z",
     "shell.execute_reply.started": "2025-09-16T17:20:03.900963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Preprocessing ---\n",
    "transform_list = [transforms.Resize((IMG_HEIGHT, IMG_WIDTH)), transforms.ToTensor()]\n",
    "if USE_IMAGENET_NORM:\n",
    "    transform_list.append(transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                               std=[0.229, 0.224, 0.225]))\n",
    "transform = transforms.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T17:20:03.906326Z",
     "iopub.status.busy": "2025-09-16T17:20:03.905978Z",
     "iopub.status.idle": "2025-09-16T17:20:03.920298Z",
     "shell.execute_reply": "2025-09-16T17:20:03.919661Z",
     "shell.execute_reply.started": "2025-09-16T17:20:03.906296Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Perspective Transform ---\n",
    "def get_perspective_transform(frame_w, frame_h):\n",
    "    src = np.float32([\n",
    "        [frame_w*0.45, frame_h*0.6],\n",
    "        [frame_w*0.55, frame_h*0.6],\n",
    "        [frame_w*0.1, frame_h*0.95],\n",
    "        [frame_w*0.9, frame_h*0.95]\n",
    "    ])\n",
    "    dst = np.float32([\n",
    "        [frame_w*0.25, 0],\n",
    "        [frame_w*0.75, 0],\n",
    "        [frame_w*0.25, frame_h],\n",
    "        [frame_w*0.75, frame_h]\n",
    "    ])\n",
    "    return cv2.getPerspectiveTransform(src, dst), cv2.getPerspectiveTransform(dst, src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T17:20:03.921204Z",
     "iopub.status.busy": "2025-09-16T17:20:03.920927Z",
     "iopub.status.idle": "2025-09-16T17:20:03.937235Z",
     "shell.execute_reply": "2025-09-16T17:20:03.936585Z",
     "shell.execute_reply.started": "2025-09-16T17:20:03.921184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fit_polynomial(mask):\n",
    "    h, w = mask.shape\n",
    "    ys, xs = np.where(mask > 0)\n",
    "    if len(ys) < 100:\n",
    "        return None, None\n",
    "    midpoint = w // 2\n",
    "    left_ys, left_xs = ys[xs < midpoint], xs[xs < midpoint]\n",
    "    right_ys, right_xs = ys[xs >= midpoint], xs[xs >= midpoint]\n",
    "    left_fit, right_fit = None, None\n",
    "    if len(left_ys) > 100:\n",
    "        left_fit = np.polyfit(left_ys, left_xs, 2)\n",
    "    if len(right_ys) > 100:\n",
    "        right_fit = np.polyfit(right_ys, right_xs, 2)\n",
    "    return left_fit, right_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T17:20:03.938996Z",
     "iopub.status.busy": "2025-09-16T17:20:03.938784Z",
     "iopub.status.idle": "2025-09-16T17:25:01.748312Z",
     "shell.execute_reply": "2025-09-16T17:25:01.747500Z",
     "shell.execute_reply.started": "2025-09-16T17:20:03.938981Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing video...\n",
      "✅ Video processing with smoothing complete. Saved to /kaggle/working/final-output-clip4.mp4\n"
     ]
    }
   ],
   "source": [
    "# --- Main Video Loop (MODIFIED to use smoothing) ---\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "frame_w, frame_h = int(cap.get(3)), int(cap.get(4))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "fps = fps if fps > 0 else 30\n",
    "out = cv2.VideoWriter(OUTPUT_VIDEO_PATH, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_w, frame_h))\n",
    "\n",
    "M, Minv = get_perspective_transform(frame_w, frame_h)\n",
    "lane_history = LaneHistory()\n",
    "\n",
    "print(\"\\nProcessing video...\")\n",
    "frame_count = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # print(f\"Processing frame {frame_count}...\")\n",
    "\n",
    "    # --- Preprocess + Predict (same as before) ---\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    image_tensor = transform(pil_image).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        pred = torch.sigmoid(model(image_tensor))\n",
    "        pred = (pred > THRESHOLD).float()\n",
    "    pred_mask = pred.squeeze().cpu().numpy()\n",
    "    pred_mask = cv2.resize(pred_mask, (frame_w, frame_h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # print(\"Prediction done.\")\n",
    "\n",
    "    # --- Warp, Fit, and Smooth ---\n",
    "    warped_mask = cv2.warpPerspective(pred_mask, M, (frame_w, frame_h))\n",
    "    current_left_fit, current_right_fit = fit_polynomial(warped_mask)\n",
    "\n",
    "    # print(f\"  - Current fits: L={current_left_fit is not None}, R={current_right_fit is not None}\")\n",
    "\n",
    "    lane_history.add_fit(current_left_fit, current_right_fit)\n",
    "    avg_left_fit, avg_right_fit = lane_history.get_average_fit()\n",
    "\n",
    "    # print(f\"  - Average fits: L={avg_left_fit is not None}, R={avg_right_fit is not None}\")\n",
    "\n",
    "    # --- Draw overlay and warnings using the SMOOTHED fit ---\n",
    "    final_frame = frame.copy()\n",
    "    y_range = np.linspace(0, frame_h - 1, frame_h)\n",
    "\n",
    "    overlay = frame.copy()\n",
    "    lane_detected = False\n",
    "\n",
    "    if avg_left_fit is not None:\n",
    "        left_avg_x = avg_left_fit[0]*y_range**2 + avg_left_fit[1]*y_range + avg_left_fit[2]\n",
    "        pts_left = np.array([np.transpose(np.vstack([left_avg_x, y_range]))]).astype(np.float32)\n",
    "        unwarped_left = cv2.perspectiveTransform(pts_left, Minv).astype(np.int32)\n",
    "        cv2.polylines(overlay, [unwarped_left], isClosed=False, color=(0, 255, 255), thickness=8)\n",
    "        lane_detected = True\n",
    "\n",
    "    if avg_right_fit is not None:\n",
    "        right_avg_x = avg_right_fit[0]*y_range**2 + avg_right_fit[1]*y_range + avg_right_fit[2]\n",
    "        pts_right = np.array([np.transpose(np.vstack([right_avg_x, y_range]))]).astype(np.float32)\n",
    "        unwarped_right = cv2.perspectiveTransform(pts_right, Minv).astype(np.int32)\n",
    "        cv2.polylines(overlay, [unwarped_right], isClosed=False, color=(255, 255, 0), thickness=8)\n",
    "        lane_detected = True\n",
    "\n",
    "    if avg_left_fit is not None and avg_right_fit is not None:\n",
    "        # Draw filled polygon between lanes\n",
    "        pts_left = np.array([np.transpose(np.vstack([left_avg_x, y_range]))])\n",
    "        pts_right = np.array([np.flipud(np.transpose(np.vstack([right_avg_x, y_range])))])\n",
    "        pts = np.hstack((pts_left, pts_right)).astype(np.float32)\n",
    "        unwarped_pts = cv2.perspectiveTransform(pts, Minv).astype(np.int32)\n",
    "        cv2.fillPoly(overlay, [unwarped_pts], (0, 255, 0))\n",
    "\n",
    "        # Lane departure warning\n",
    "        y_bottom = frame_h - 1\n",
    "        left_x_bottom = np.polyval(avg_left_fit, y_bottom)\n",
    "        right_x_bottom = np.polyval(avg_right_fit, y_bottom)\n",
    "        lane_center = (left_x_bottom + right_x_bottom) / 2\n",
    "        deviation = abs(frame_w/2 - lane_center)\n",
    "        lane_width = abs(right_x_bottom - left_x_bottom)\n",
    "\n",
    "        warning_text = \"Safe\" if deviation <= lane_width * 0.35 else \"⚠ Lane Departure!\"\n",
    "        color = (0, 0, 255) if \"⚠\" in warning_text else (0, 255, 0)\n",
    "    elif lane_detected:\n",
    "        # Only one lane detected → partial support\n",
    "        warning_text = \"Partial Lane Detected\"\n",
    "        color = (0, 165, 255)  # orange\n",
    "    else:\n",
    "        warning_text = \"WARNING: Lane Lost!\"\n",
    "        color = (0, 0, 255)\n",
    "\n",
    "    # Blend overlay\n",
    "    final_frame = cv2.addWeighted(frame, 1, overlay, 0.4 if lane_detected else 0, 0)\n",
    "\n",
    "    # Draw warning box + text\n",
    "    cv2.rectangle(final_frame, (40, 15), (420, 70), (0, 0, 0), -1)\n",
    "    cv2.putText(final_frame, warning_text, (50, 55),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.3, color, 3, cv2.LINE_AA)\n",
    "\n",
    "    # print(\"Overlay and warnings drawn.\\n\")\n",
    "    out.write(final_frame)\n",
    "    # print(\"Frame written to output.\")\n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"✅ Video processing with smoothing complete. Saved to {OUTPUT_VIDEO_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8191445,
     "sourceId": 12983363,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8282382,
     "sourceId": 13077340,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
